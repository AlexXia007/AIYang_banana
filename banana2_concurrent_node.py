"""
Banana2 Concurrent Node
ç‹¬ç«‹çš„ Banana2 å¹¶å‘å›¾åƒç”ŸæˆèŠ‚ç‚¹
æ”¯æŒå¤šå›¾è¾“å…¥ã€å¹¶å‘è¯·æ±‚ã€é‡è¯•æœºåˆ¶å’Œè¶…æ—¶æ§åˆ¶
"""

import os
import io
import json
import base64
import requests
import time
import uuid
import datetime
import torch
import numpy as np
from PIL import Image
from io import BytesIO
import re
import mimetypes
import wave
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections.abc import Mapping
from typing import Tuple, Optional, List

import oss2


def _log(message):
    """æ—¥å¿—è¾“å‡º"""
    print(f"[Banana2-Concurrent] {message}")


def _auto_auth_headers(base_url: str, api_key: str, auth_mode: str):
    """æ„å»ºè®¤è¯å¤´"""
    headers = {"Content-Type": "application/json"}
    mode = (auth_mode or "auto").lower()
    if mode == "google_xgoog" or (mode == "auto" and "generativelanguage.googleapis.com" in (base_url or "")):
        headers["x-goog-api-key"] = api_key
    else:
        headers["Authorization"] = f"Bearer {api_key}"
    return headers


def _build_endpoint(base_url: str, model: str, version: str):
    """æ„å»º API ç«¯ç‚¹ URL"""
    u = (base_url or "").rstrip('/')
    if "/models/" in u and ":generateContent" in u:
        return u

    # Check if base_url already contains a version path
    if u.endswith('/v1') or u.endswith('/v1beta') or u.endswith('/v1alpha'):
        return f"{u}/models/{model}:generateContent"

    ver = (version or "Auto").lower()
    if ver == "auto":
        ver = "v1beta" if "generativelanguage.googleapis.com" in u else "v1"

    return f"{u}/{ver}/models/{model}:generateContent"


def _deep_merge(dst: dict, src: dict):
    """æ·±åº¦åˆå¹¶å­—å…¸"""
    for k, v in (src or {}).items():
        if isinstance(v, dict) and isinstance(dst.get(k), dict):
            _deep_merge(dst[k], v)
        else:
            dst[k] = v
    return dst


def _redact_for_log(obj, max_len=256):
    """æ—¥å¿—è„±æ•ï¼šéšè—å¤§æ®µ base64 æ•°æ®"""
    def is_base64_like(s: str) -> bool:
        try:
            return bool(re.fullmatch(r"[A-Za-z0-9+/=\n\r]+", s))
        except Exception:
            return False

    def walk(v):
        if isinstance(v, dict):
            out = {}
            for k, val in v.items():
                if k == "data" and isinstance(val, str) and len(val) > max_len:
                    out[k] = f"[redacted {len(val)} chars]"
                else:
                    out[k] = walk(val)
            return out
        if isinstance(v, list):
            return [walk(x) for x in v]
        if isinstance(v, str):
            if len(v) > max_len and is_base64_like(v):
                return f"[redacted {len(v)} chars]"
            if len(v) > 4096:
                return v[:1024] + f"... [truncated, total {len(v)} chars]"
            return v
        return v

    try:
        return walk(obj)
    except Exception:
        return obj


# å›¾ç‰‡ä¸‹è½½ç¼“å­˜ï¼ˆé¿å…å¤šä¸ªä»»åŠ¡é‡å¤ä¸‹è½½ç›¸åŒå›¾ç‰‡ï¼‰
_image_cache = {}
_image_cache_lock = None


class OSSUploadFromData:
    def _build_object_key(self, suggested_name: str, prefix: str) -> str:
        today = datetime.datetime.utcnow()
        date_path = f"{today.year:04d}/{today.month:02d}/{today.day:02d}"
        base = suggested_name.strip() or f"file_{uuid.uuid4().hex[:8]}.bin"
        base = base.replace("\\", "/").split("/")[-1]
        key = "/".join(x.strip("/\\") for x in [prefix, date_path, base] if x)
        return key.replace("\\", "/")

    def _numpy_to_pil(self, arr: np.ndarray) -> Image.Image:
        """
        å°†å•å¼ å›¾åƒçš„ numpy æ•°ç»„å®‰å…¨è½¬æ¢ä¸º PILï¼Œå…¼å®¹ HWC / CHWï¼Œå¹¶ä¿ç•™é€æ˜åº¦ã€‚
        """
        if arr.ndim == 2:
            # ç°åº¦
            return Image.fromarray(arr.astype(np.uint8), mode="L")

        if arr.ndim != 3:
            raise RuntimeError(f"Unsupported image array shape: {arr.shape}")

        h, w, c = arr.shape

        # å¦‚æœæ˜¯ [C, H, W]ï¼Œåˆ™è½¬ä¸º [H, W, C]
        if c not in (1, 3, 4) and h in (1, 3, 4):
            arr = np.transpose(arr, (1, 2, 0))
            h, w, c = arr.shape

        arr = arr.astype(np.uint8, copy=False)

        if c == 4:
            return Image.fromarray(arr, mode="RGBA")
        if c == 3:
            return Image.fromarray(arr, mode="RGB")
        if c == 1:
            return Image.fromarray(arr[:, :, 0], mode="L")

        # éå¸¸è§„é€šé“æ•°ï¼Œäº¤ç»™ PIL è‡ªè¡Œæ¨æ–­
        return Image.fromarray(arr)

    def _img_batch_to_payload(self, image: torch.Tensor) -> List[Tuple[bytes, str, str]]:
        image = image.clamp(0, 1)
        batch = image.shape[0] if len(image.shape) == 4 else 1

        payloads: List[Tuple[bytes, str, str]] = []

        # å•å¼ å›¾
        if batch == 1:
            uid = uuid.uuid4().hex[:8]
            arr = (
                (image[0].cpu().numpy() * 255).astype(np.uint8)
                if len(image.shape) == 4
                else (image.cpu().numpy() * 255).astype(np.uint8)
            )
            pil = self._numpy_to_pil(arr)
            bio = io.BytesIO()
            pil.save(bio, format="PNG")
            payloads.append((bio.getvalue(), f"image_{uid}.png", "image/png"))
            return payloads

        # å¤šå¼ å›¾ -> å¤šä¸ª PNGï¼Œåˆ†åˆ«ä¸Šä¼ 
        uid = uuid.uuid4().hex[:8]
        for i in range(batch):
            arr = (image[i].cpu().numpy() * 255).astype(np.uint8)
            pil = self._numpy_to_pil(arr)
            f_bio = io.BytesIO()
            pil.save(f_bio, format="PNG")
            name = f"image_{uid}_{i+1:04d}.png"
            payloads.append((f_bio.getvalue(), name, "image/png"))
        return payloads

    def _audio_input_to_bytes(self, audio: object, file_name: str, mime_type: str) -> Tuple[bytes, str, str]:
        # 0) Already bytes
        if isinstance(audio, (bytes, bytearray)):
            name = file_name.strip() or f"audio_{uuid.uuid4().hex[:8]}.wav"
            mt = mime_type.strip() or (mimetypes.guess_type(name)[0] or "audio/wav")
            return (bytes(audio), name, mt)

        # 1) Try common file path attributes
        potential_path = None
        for attr in ("file", "path", "file_path", "filepath", "audio_path", "filename"):
            if hasattr(audio, attr):
                val = getattr(audio, attr)
                if isinstance(val, str) and os.path.isfile(val):
                    potential_path = val
                    break
        if potential_path is None and isinstance(audio, str) and os.path.isfile(audio):
            potential_path = audio
        if potential_path:
            with open(potential_path, "rb") as f:
                data = f.read()
            name = file_name.strip() or os.path.basename(potential_path)
            mt = mime_type.strip() or (mimetypes.guess_type(name)[0] or "application/octet-stream")
            return data, name, mt

        # 2) Try common export methods to get wav bytes
        for meth in ("to_wav_bytes", "get_wav_bytes"):
            fn = getattr(audio, meth, None)
            if callable(fn):
                try:
                    data = fn()
                    if isinstance(data, (bytes, bytearray)):
                        name = file_name.strip() or f"audio_{uuid.uuid4().hex[:8]}.wav"
                        mt = mime_type.strip() or "audio/wav"
                        return bytes(data), name, mt
                except Exception:
                    pass
        for meth in ("export", "save", "write"):
            fn = getattr(audio, meth, None)
            if callable(fn):
                try:
                    bio = io.BytesIO()
                    try:
                        fn(bio, format="wav")
                    except Exception:
                        fn(bio)
                    data = bio.getvalue()
                    if data:
                        name = file_name.strip() or f"audio_{uuid.uuid4().hex[:8]}.wav"
                        mt = mime_type.strip() or "audio/wav"
                        return data, name, mt
                except Exception:
                    pass

        # 3) Treat as waveform tensor/array
        sr = 44100
        data = None
        if isinstance(audio, Mapping):
            # LazyAudioMap implements Mapping and resolves on first access
            # Fetch sample rate without boolean evaluation on tensors
            for k in ("sample_rate", "sr"):
                try:
                    v = audio.get(k)  # type: ignore[attr-defined]
                    if v is not None:
                        sr = int(v)
                        break
                except Exception:
                    pass
            for k in ("samples", "waveform", "audio"):
                try:
                    v = audio.get(k)  # type: ignore[attr-defined]
                    if v is not None:
                        data = v
                        break
                except Exception:
                    continue
        else:
            data = audio

        # 2.5) Attribute-style containers (e.g., objects with .waveform / .sample_rate)
        if data is audio and not isinstance(audio, (bytes, bytearray)) and not isinstance(audio, Mapping):
            try:
                sr_attr = getattr(audio, "sample_rate", None)
                wf_attr = getattr(audio, "waveform", None)
                if sr_attr is not None and wf_attr is not None:
                    try:
                        sr = int(sr_attr)
                    except Exception:
                        pass
                    data = wf_attr
            except Exception:
                pass

        if isinstance(data, torch.Tensor):
            data_np = data.detach().cpu().numpy()
        else:
            try:
                data_np = np.asarray(data)
            except Exception:
                data_np = None

        if data_np is None or not np.issubdtype(getattr(data_np, "dtype", np.float32), np.number):
            raise RuntimeError(
                "Unsupported AUDIO input: cannot extract waveform or file path from object. "
                "Provide a numeric waveform, a valid file path, or an object with export methods."
            )

        if data_np.ndim == 3 and data_np.shape[0] == 1:
            # [1, C, S] -> [C, S]
            data_np = data_np[0]
        if data_np.ndim == 1:
            data_np = data_np[None, :]
        elif data_np.ndim != 2:
            raise RuntimeError(f"Unsupported audio array shape: {data_np.shape}")

        data_np = data_np.astype(np.float32, copy=False)
        data_np = np.clip(data_np, -1.0, 1.0)
        pcm_i16 = (data_np * 32767.0).astype(np.int16)
        frames = pcm_i16.T.tobytes()

        bio = io.BytesIO()
        with wave.open(bio, "wb") as wf:
            wf.setnchannels(pcm_i16.shape[0])
            wf.setsampwidth(2)
            wf.setframerate(sr)
            wf.writeframes(frames)
        name = file_name.strip() or f"audio_{uuid.uuid4().hex[:8]}.wav"
        mt = mime_type.strip() or "audio/wav"
        return bio.getvalue(), name, mt

    def _audio_many_to_payloads(self, audios: object, file_name: str, mime_type: str) -> List[Tuple[bytes, str, str]]:
        """
        æ”¯æŒå•ä¸ªéŸ³é¢‘å¯¹è±¡æˆ–å¤šä¸ªéŸ³é¢‘å¯¹è±¡ï¼ˆlist/tuple ç­‰å¯è¿­ä»£ï¼‰è½¬æ¢ä¸ºç»Ÿä¸€çš„ payload åˆ—è¡¨ã€‚
        """
        if isinstance(audios, (list, tuple)):
            payloads: List[Tuple[bytes, str, str]] = []
            for a in audios:
                payloads.append(self._audio_input_to_bytes(a, file_name, mime_type))
            return payloads
        # é€€åŒ–ä¸ºå•ä¸ª
        return [self._audio_input_to_bytes(audios, file_name, mime_type)]

    def _video_input_to_bytes(self, video: object, file_name: str, mime_type: str) -> Tuple[bytes, str, str]:
        """
        å°†å•ä¸ªè§†é¢‘å¯¹è±¡æˆ–è·¯å¾„è½¬æ¢ä¸º payloadï¼ˆä¸‰å…ƒç»„ï¼‰ã€‚
        """
        potential_path = None
        for attr in ("file", "path", "file_path", "filepath", "fullpath", "filename"):
            if hasattr(video, attr):
                val = getattr(video, attr)
                if isinstance(val, str) and os.path.isfile(val):
                    potential_path = val
                    break
        if potential_path is None and isinstance(video, str) and os.path.isfile(video):
            potential_path = video
        if potential_path is None:
            raise RuntimeError(
                "Unsupported VIDEO input: cannot resolve file path from object. "
                "Provide a valid file path or object with path attributes."
            )

        with open(potential_path, "rb") as f:
            data = f.read()
        name = file_name.strip() or os.path.basename(potential_path)
        mt = mime_type.strip() or (mimetypes.guess_type(name)[0] or "application/octet-stream")
        return data, name, mt

    def _video_many_to_payloads(self, videos: object, file_name: str, mime_type: str) -> List[Tuple[bytes, str, str]]:
        """
        æ”¯æŒå•ä¸ªè§†é¢‘å¯¹è±¡æˆ–å¤šä¸ªè§†é¢‘å¯¹è±¡ï¼ˆlist/tuple ç­‰å¯è¿­ä»£ï¼‰è½¬æ¢ä¸º payload åˆ—è¡¨ã€‚
        """
        if isinstance(videos, (list, tuple)):
            payloads: List[Tuple[bytes, str, str]] = []
            for v in videos:
                payloads.append(self._video_input_to_bytes(v, file_name, mime_type))
            return payloads
        return [self._video_input_to_bytes(videos, file_name, mime_type)]

    def _choose_payloads(
        self,
        image: Optional[torch.Tensor],
        images: Optional[torch.Tensor],
        audio: Optional[object],
        audios: Optional[object],
        video: Optional[object],
        videos: Optional[object],
        file_name: str,
        mime_type: str,
    ) -> List[Tuple[bytes, str, str]]:
        """
        æ ¹æ®ä¼˜å…ˆçº§é€‰æ‹©å¾…ä¸Šä¼ çš„è½½è·ã€‚

        ä¼˜å…ˆçº§ï¼š
        1. imagesï¼ˆç»„å›¾ï¼‰
        2. imageï¼ˆå•å›¾ï¼‰
        3. audiosï¼ˆå¤šéŸ³é¢‘ï¼‰
        4. audioï¼ˆå•éŸ³é¢‘ï¼‰
        5. videosï¼ˆå¤šè§†é¢‘ï¼‰
        6. videoï¼ˆå•è§†é¢‘ï¼‰
        """
        # 1) å›¾ç‰‡ï¼šä¼˜å…ˆä½¿ç”¨ç»„å›¾ç«¯å£
        if images is not None:
            return self._img_batch_to_payload(images)
        if image is not None:
            return self._img_batch_to_payload(image)

        # 2) éŸ³é¢‘ â†’ WAV
        if audios is not None:
            return self._audio_many_to_payloads(audios, file_name, mime_type)
        if audio is not None:
            return [self._audio_input_to_bytes(audio, file_name, mime_type)]

        # 3) è§†é¢‘
        if videos is not None:
            return self._video_many_to_payloads(videos, file_name, mime_type)
        if video is not None:
            return [self._video_input_to_bytes(video, file_name, mime_type)]
        # æ— æœ‰æ•ˆè½½è·
        raise RuntimeError("No payload provided. Connect one of: image, audio, or video.")

    def _to_public_url(self, endpoint: str, bucket_name: str, object_key: str) -> str:
        scheme = "https"
        ep = endpoint
        if endpoint.startswith("http://"):
            scheme = "http"
            ep = endpoint[len("http://") :]
        elif endpoint.startswith("https://"):
            ep = endpoint[len("https://") :]
        return f"{scheme}://{bucket_name}.{ep}/{object_key}"

    def upload(
        self,
        endpoint: str,
        access_key_id: str,
        access_key_secret: str,
        bucket_name: str,
        object_prefix: str,
        use_signed_url: bool,
        signed_url_expire_seconds: int,
        image: Optional[torch.Tensor] = None,
        images: Optional[torch.Tensor] = None,
        audio: Optional[object] = None,
        audios: Optional[object] = None,
        video: Optional[object] = None,
        videos: Optional[object] = None,
        file_name: str = "",
        mime_type: str = "",
        security_token: str = "",
    ):
        if not endpoint or not access_key_id or not access_key_secret or not bucket_name:
            raise RuntimeError("Missing required OSS configuration.")

        payloads = self._choose_payloads(
            image=image,
            images=images,
            audio=audio,
            audios=audios,
            video=video,
            videos=videos,
            file_name=file_name,
            mime_type=mime_type,
        )

        auth = (
            oss2.StsAuth(access_key_id, access_key_secret, security_token)
            if security_token
            else oss2.Auth(access_key_id, access_key_secret)
        )
        bucket = oss2.Bucket(auth, endpoint, bucket_name)

        urls: List[str] = []

        for payload, suggested_name, content_type in payloads:
            object_key = self._build_object_key(suggested_name, object_prefix)
            headers = {"Content-Type": content_type}
            result = bucket.put_object(object_key, payload, headers=headers)
            if not (200 <= result.status < 300):
                raise RuntimeError(f"Upload failed: status={result.status}")

            url = (
                bucket.sign_url("GET", object_key, signed_url_expire_seconds)
                if use_signed_url
                else self._to_public_url(endpoint, bucket_name, object_key)
            )
            urls.append(url)

        return (urls,)

def _init_cache_lock():
    """åˆå§‹åŒ–ç¼“å­˜é”ï¼ˆå»¶è¿Ÿå¯¼å…¥ threadingï¼‰"""
    global _image_cache_lock
    if _image_cache_lock is None:
        import threading
        _image_cache_lock = threading.Lock()
    return _image_cache_lock

def _download_image(url: str, proxies=None, timeout=120, use_cache=True):
    """ä¸‹è½½å›¾ç‰‡ï¼ˆå¸¦ç¼“å­˜ï¼Œçº¿ç¨‹å®‰å…¨ï¼‰"""
    # ğŸ”§ ä¼˜åŒ–ï¼šåŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ï¼Œé¿å…ç«æ€æ¡ä»¶
    if use_cache:
        lock = _init_cache_lock()
        # ç¬¬ä¸€æ¬¡æ£€æŸ¥ï¼ˆä¸åŠ é”ï¼Œå¿«é€Ÿè·¯å¾„ï¼‰
        if url in _image_cache:
            _log(f"Using cached image: {url[:50]}...")
            return _image_cache[url]
        
        # ç¬¬äºŒæ¬¡æ£€æŸ¥ï¼ˆåŠ é”ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨ï¼‰
        with lock:
            if url in _image_cache:
                _log(f"Using cached image (locked): {url[:50]}...")
                return _image_cache[url]
    
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹ä¸‹è½½
    try:
        _log(f"Downloading image: {url[:50]}...")
        r = requests.get(url, timeout=timeout, proxies=proxies)
        if r.status_code == 200:
            img_data = r.content
            # ç¼“å­˜å›¾ç‰‡æ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
            if use_cache:
                lock = _init_cache_lock()
                with lock:
                    # å†æ¬¡æ£€æŸ¥ï¼Œé¿å…é‡å¤å†™å…¥ï¼ˆè™½ç„¶å·²ç»ä¸‹è½½äº†ï¼‰
                    if url not in _image_cache:
                        _image_cache[url] = img_data
                        _log(f"Cached image: {url[:50]}... ({len(img_data)} bytes)")
                    else:
                        _log(f"Image already cached by another thread: {url[:50]}...")
            return img_data
        _log(f"Download failed: HTTP {r.status_code}")
    except Exception as e:
        _log(f"Error downloading image: {e}")
    return None


def _extract_response_images(resp_json, strict_native=False, proxies=None, timeout=120):
    """
    ä»å“åº”ä¸­æå–æ‰€æœ‰å¯ç”¨çš„å›¾ç‰‡ã€‚
    è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸ï¼š{"bytes": b"...", "mime": "image/png", "url": optional str}
    """
    images = []
    seen_urls = set()

    def add_inline(data_str, mime):
        if not data_str:
            return
        try:
            decoded = base64.b64decode(data_str)
            images.append({
                "bytes": decoded,
                "mime": mime or "image/png",
                "url": None,
            })
        except Exception as err:
            _log(f"âš ï¸ è§£ç  inline å›¾åƒå¤±è´¥: {err}")

    def add_url_resource(url):
        if not url or url in seen_urls:
            return
        seen_urls.add(url)
        img_data = _download_image(url, proxies=proxies, timeout=timeout, use_cache=True)
        if not img_data:
            _log(f"âŒ ä¸‹è½½å›¾åƒå¤±è´¥: {url}")
            return
        mime = mimetypes.guess_type(url)[0] or "image/png"
        images.append({
            "bytes": img_data,
            "mime": mime,
            "url": url,
        })

    # 1) Gemini style: candidates -> parts
    try:
        cands = resp_json.get("candidates") or []
        for cand in cands:
            parts = (cand.get("content") or {}).get("parts") or []
            for p in parts:
                data = p.get("inlineData") or p.get("inline_data") or {}
                mt = (data.get("mimeType") or data.get("mime_type") or "")
                if isinstance(mt, str) and mt.startswith("image/"):
                    add_inline(data.get("data"), mt)

                if not strict_native:
                    text = p.get("text") or ""
                    if text:
                        _log(f"ğŸ” æ£€æŸ¥æ–‡æœ¬ä¸­çš„å›¾åƒURL: {text[:200]}")
                        for match in re.findall(r'!\[[^\]]*\]\((https?://[^\)]+)\)', text):
                            add_url_resource(match.strip())
                        for match in re.findall(r'(https?://[^\s\)]+\.(?:png|jpg|jpeg|gif|webp|bmp))', text, re.IGNORECASE):
                            add_url_resource(match.strip())
    except Exception as e:
        _log(f"Error in Gemini-style image extraction: {e}")
        import traceback
        _log(traceback.format_exc())

    # 2) OpenAI / DALLÂ·E style data[]
    try:
        data_list = resp_json.get("data")
        if isinstance(data_list, list):
            for item in data_list:
                b64 = item.get("b64_json")
                if b64:
                    add_inline(b64, item.get("mimeType") or "image/png")
                url = item.get("url")
                if url:
                    add_url_resource(url)
    except Exception as e:
        _log(f"Error in OpenAI-style image extraction: {e}")

    # 3) Generic fallbacks (image/images)
    try:
        for k in ["image", "images"]:
            v = resp_json.get(k)
            if isinstance(v, list):
                for item in v:
                    b64 = item.get("base64") or item.get("b64") or item.get("data")
                    if b64:
                        add_inline(b64, item.get("mimeType") or "image/png")
                    url = item.get("url")
                    if url:
                        add_url_resource(url)
    except Exception as e:
        _log(f"Error in fallback image extraction: {e}")

    return images


def _load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä»…è¯»å–æœ¬ç›®å½•å†…çš„é…ç½®æ–‡ä»¶ï¼‰"""
    try:
        # åªè¯»å–å½“å‰æ’ä»¶ç›®å½•å†…çš„é…ç½®æ–‡ä»¶
        current_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(current_dir, "config.json")
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        _log(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    # è¿”å›é»˜è®¤ç©ºé…ç½®
    return {}


def _get_mirror_site_config(mirror_site_name: str):
    """æ ¹æ®é•œåƒç«™åç§°è·å–å¯¹åº”çš„ url ä¸ api_key"""
    config = _load_config()
    sites = config.get('mirror_sites', {}) or {}
    if mirror_site_name and mirror_site_name.lower() != 'custom' and mirror_site_name in sites:
        site = sites.get(mirror_site_name, {})
        return {
            'url': site.get('url', ''),
            'api_key': site.get('api_key', '')
        }, config
    return {'url': '', 'api_key': ''}, config


class Banana2ConcurrentNode:
    """Banana2 å¹¶å‘å›¾åƒç”ŸæˆèŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        # ä»é…ç½®æ–‡ä»¶è¯»å–é•œåƒç«™é€‰é¡¹
        config = _load_config()
        mirror_sites = config.get('mirror_sites', {}) or {}
        mirror_options = list(mirror_sites.keys())
        # ç»Ÿä¸€åŒ…å«è‡ªå®šä¹‰é€‰é¡¹
        mirror_options = ["Custom" if x.lower() == "custom" else x for x in mirror_options]
        if "Custom" not in mirror_options:
            mirror_options.append("Custom")

        # é»˜è®¤é•œåƒç«™ï¼šä¼˜å…ˆ nano-bananaå®˜æ–¹ï¼Œå…¶æ¬¡ comflyï¼Œå†æ¬¡ç¬¬ä¸€ä¸ªï¼Œæœ€å Custom
        if "nano-bananaå®˜æ–¹" in mirror_options:
            default_site = "nano-bananaå®˜æ–¹"
        elif "comfly" in mirror_options:
            default_site = "comfly"
        elif mirror_options:
            default_site = mirror_options[0]
        else:
            default_site = "Custom"

        return {
            "required": {
                # æç¤ºè¯æ–‡æœ¬æ¡†
                "prompt": ("STRING", {"default": "ç”Ÿæˆä¸€å¼ æ¸…æ™°çš„é¦™æ°´äº§å“å›¾", "multiline": True}),
                # é•œåƒç«™é€‰æ‹©
                "mirror_site": (mirror_options, {"default": default_site}),
                # API è®¤è¯å‚æ•°
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "base_url": ("STRING", {"default": "https://generativelanguage.googleapis.com"}),

                # æ¨¡å‹é€‰æ‹©
                "model": ([
                    "gemini-3-pro-image-preview",
                    "custom"
                ], {"default": "gemini-3-pro-image-preview"}),
                "custom_model": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "å½“modelé€‰æ‹©'custom'æ—¶ï¼Œåœ¨æ­¤è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹åç§°"
                }),
                "version": (["Auto", "v1", "v1alpha", "v1beta"], {"default": "Auto"}),
                "auth_mode": (["auto", "google_xgoog", "bearer"], {"default": "auto"}),
                "response_mode": (["TEXT_AND_IMAGE", "IMAGE_ONLY", "TEXT_ONLY"], {"default": "TEXT_AND_IMAGE"}),
                "aspect_ratio": (["Auto","1:1","16:9","9:16","4:3","3:4","3:2","2:3","5:4","4:5","21:9"], {"default": "Auto"}),
                "image_size": (["Auto","1K","2K","4K"], {"default": "Auto"}),

                # æŒ‰é¡ºåºï¼štemperature -> top_p -> top_k -> max_output_tokens
                "temperature": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 2.0}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": 40, "min": 1, "max": 1000}),
                "max_output_tokens": ("INT", {"default": 2048, "min": 1, "max": 32768}),

                "seed": ("INT", {"default": 0, "min": 0, "max": 2147483647}),
                "strict_native": ("BOOLEAN", {"default": False}),
                "system_instruction": ("STRING", {"default": "", "multiline": True}),
                "image_mime": (["image/png","image/jpeg","image/webp"], {"default": "image/png"}),
                
                # å¹¶å‘ä¸é‡è¯•æ§åˆ¶
                "concurrency": ("INT", {"default": 3, "min": 1, "max": 100, "tooltip": "åŒæ—¶å¹¶å‘è¯·æ±‚æ•°é‡"}),
                "request_delay": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 10.0, "step": 0.1, "tooltip": "å¹¶å‘è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿé—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…ç¬é—´åŒæ—¶å‘é€"}),
                "retry_times": ("INT", {"default": 1, "min": 1, "max": 10, "tooltip": "å•æ¬¡è¯·æ±‚å¤±è´¥åé¢å¤–é‡è¯•æ¬¡æ•°"}),
                "single_timeout": ("INT", {"default": 300, "min": 10, "max": 5000, "tooltip": "å•æ¬¡è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"}),
                "total_timeout": ("INT", {"default": 600, "min": 10, "max": 5000, "tooltip": "æ•´ä¸ªå¹¶å‘+é‡è¯•è¿‡ç¨‹çš„æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰"}),
            },
            "optional": {
                # å›¾ç‰‡URLæ–‡æœ¬è¾“å…¥ï¼Œå¤šè¡Œï¼Œæ¯è¡Œä¸€ä¸ªURL
                "image_urls_text": ("STRING", {"default": "", "multiline": True, "tooltip": "æ¯è¡Œä¸€ä¸ªå›¾ç‰‡URLï¼Œæ”¯æŒå¤šå›¾"}),
                "extra_payload_json": ("STRING", {"default": "", "multiline": True}),
                # OSS è®¾ç½®
                "oss_enable_upload": ("BOOLEAN", {"default": False, "tooltip": "å‹¾é€‰åå°†ç”Ÿæˆçš„å›¾ç‰‡ä¸Šä¼ åˆ°æŒ‡å®š OSS"}),
                "oss_endpoint": ("STRING", {"default": "", "multiline": False}),
                "oss_access_key_id": ("STRING", {"default": "", "password": True}),
                "oss_access_key_secret": ("STRING", {"default": "", "password": True}),
                "oss_bucket_name": ("STRING", {"default": "", "multiline": False}),
                "oss_object_prefix": ("STRING", {"default": "uploads/"}),
                "oss_file_name": ("STRING", {"default": "", "multiline": False, "tooltip": "ä¸Šä¼ åˆ° OSS æ—¶ä½¿ç”¨çš„æ–‡ä»¶åï¼ˆç•™ç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰"}),
                "oss_mime_type": ("STRING", {"default": "", "multiline": False, "tooltip": "ä¸Šä¼ åˆ° OSS æ—¶ä½¿ç”¨çš„ MIME ç±»å‹ï¼ˆç•™ç©ºè‡ªåŠ¨ä½¿ç”¨ image/pngï¼‰"}),
                "oss_use_signed_url": ("BOOLEAN", {"default": True}),
                "oss_signed_url_expire_seconds": ("INT", {"default": 3600, "min": 60, "max": 604800}),
                "oss_security_token": ("STRING", {"default": "", "password": True}),
            },
        }

    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING", "IMAGE")
    RETURN_NAMES = ("responses", "statuses", "image_urls", "valid_urls", "images")
    FUNCTION = "call_api"
    CATEGORY = "AIYang007_banana"

    def call_api(self, prompt, mirror_site, api_key, base_url, model, custom_model, version, auth_mode,
                 response_mode, aspect_ratio, image_size,
                 temperature, top_p, top_k, max_output_tokens, seed, strict_native,
                 system_instruction, image_mime, concurrency, request_delay, retry_times, single_timeout, total_timeout,
                 image_urls_text="", extra_payload_json="",
                 oss_enable_upload=False, oss_endpoint="", oss_access_key_id="", oss_access_key_secret="",
                 oss_bucket_name="", oss_object_prefix="uploads/", oss_file_name="", oss_mime_type="",
                 oss_use_signed_url=True, oss_signed_url_expire_seconds=3600, oss_security_token=""):
        # è§£æé•œåƒç«™é…ç½®ä¸ç”¨æˆ·è¾“å…¥çš„ä¼˜å…ˆçº§
        site_cfg, full_cfg = _get_mirror_site_config(mirror_site)
        global_default_base = full_cfg.get('base_url', 'https://generativelanguage.googleapis.com')

        user_key = (api_key or "").strip()
        user_base = (base_url or "").strip()

        is_custom = (mirror_site or "").lower() == 'custom'
        if is_custom:
            # Custom å¿…é¡»å®Œå…¨ä¾èµ–ç”¨æˆ·è¾“å…¥
            if not user_key or not user_base:
                empty_list_json = json.dumps([], ensure_ascii=False)
                return (
                    json.dumps({"error": "é€‰æ‹© 'Custom' æ—¶å¿…é¡»è¾“å…¥ API Key å’Œ base_url"}, ensure_ascii=False),
                    json.dumps(["error"], ensure_ascii=False),
                    empty_list_json,
                    empty_list_json,
                    torch.zeros(1, 512, 512, 3),
                )
            effective_key = user_key
            effective_base = user_base
        else:
            # é Customï¼šç”¨æˆ·è¾“å…¥ä¼˜å…ˆï¼Œå¦åˆ™ä½¿ç”¨é…ç½®
            effective_key = user_key if user_key else (site_cfg.get('api_key') or full_cfg.get('api_key') or "").strip()
            effective_base = user_base if user_base else (site_cfg.get('url') or global_default_base)

            if not effective_key:
                empty_list_json = json.dumps([], ensure_ascii=False)
                return (
                    json.dumps({"error": "æœªæä¾› API Keyï¼Œä¸”é•œåƒç«™é…ç½®ä¸­ä¹Ÿæ²¡æœ‰å¯ç”¨çš„Key"}, ensure_ascii=False),
                    json.dumps(["error"], ensure_ascii=False),
                    empty_list_json,
                    empty_list_json,
                    torch.zeros(1, 512, 512, 3),
                )

        _log(f"é•œåƒç«™: {mirror_site} â†’ ä½¿ç”¨ base_url: {effective_base}")
        _log(f"è®¤è¯æ¨¡å¼: {auth_mode}")

        # ğŸ¯ å¤„ç†è‡ªå®šä¹‰æ¨¡å‹
        actual_model = model
        if model == "custom":
            if not custom_model.strip():
                empty_list_json = json.dumps([], ensure_ascii=False)
                return (
                    json.dumps({"error": "é€‰æ‹©'custom'æ—¶å¿…é¡»æä¾›è‡ªå®šä¹‰æ¨¡å‹åç§°"}, ensure_ascii=False),
                    json.dumps(["error"], ensure_ascii=False),
                    empty_list_json,
                    empty_list_json,
                    torch.zeros(1, 512, 512, 3),
                )
            actual_model = custom_model.strip()
            _log(f"ğŸ”§ ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹: {actual_model}")

        endpoint = _build_endpoint(effective_base, actual_model, version)
        headers = _auto_auth_headers(effective_base, effective_key, auth_mode)

        # è§£æå›¾ç‰‡URLåˆ—è¡¨ï¼ˆå¤šè¡Œ â†’ å¤šå›¾ï¼›è¿™äº›å›¾ä¼šä½œä¸ºä¸€ä¸ªä»»åŠ¡çš„å¤šå›¾è¾“å…¥ï¼‰
        image_urls = []
        if image_urls_text:
            for line in image_urls_text.splitlines():
                url = (line or "").strip()
                if url:
                    image_urls.append(url)

        # Build base parts: æ–‡æœ¬ prompt
        base_parts = [{"text": prompt}]

        # Base payload per Gemini docs
        base_payload = {
            "contents": [{"role": "user", "parts": base_parts}],
            "generationConfig": {
                "temperature": float(temperature),
                "topP": float(top_p),
                "topK": int(top_k),
                "maxOutputTokens": int(max_output_tokens),
            },
        }

        # responseModalities
        if response_mode == "IMAGE_ONLY":
            mods = ["IMAGE"]
        elif response_mode == "TEXT_ONLY":
            mods = ["TEXT"]
        else:
            mods = ["TEXT", "IMAGE"]
        base_payload.setdefault("generationConfig", {})["responseModalities"] = mods

        # imageConfig: aspectRatio + imageSize
        gen_cfg = base_payload.setdefault("generationConfig", {})

        if aspect_ratio and aspect_ratio != "Auto":
            gen_cfg.setdefault("imageConfig", {})["aspectRatio"] = aspect_ratio
        if image_size and image_size != "Auto":
            val = str(image_size).upper()
            gen_cfg.setdefault("imageConfig", {})["imageSize"] = val

        # seed (0 means no seed)
        try:
            if isinstance(seed, int) and seed > 0:
                base_payload.setdefault("generationConfig", {})["seed"] = int(seed)
        except Exception:
            pass

        # systemInstruction
        if system_instruction and system_instruction.strip():
            base_payload["systemInstruction"] = {
                "role": "system",
                "parts": [{"text": system_instruction.strip()}]
            }

        # Merge extra JSON
        if extra_payload_json and extra_payload_json.strip():
            try:
                user_extra = json.loads(extra_payload_json)
                base_payload = _deep_merge(base_payload, user_extra)
            except Exception as e:
                _log(f"extra_payload_json parse error: {e}")

        # ä¸åœ¨èŠ‚ç‚¹ä¸­å•ç‹¬ç®¡ç†ä»£ç†è®¾ç½®ï¼Œç›´æ¥ä½¿ç”¨ç³»ç»Ÿ/requests é»˜è®¤è¡Œä¸º
        proxies = None

        def _call_single(idx, start_time, task_start_delay=0):
            """å•ä¸ªå¹¶å‘ä»»åŠ¡çš„æ‰§è¡Œå‡½æ•°"""
            # ğŸ”§ ä»»åŠ¡å†…éƒ¨å»¶è¿Ÿï¼šåœ¨å‘é€è¯·æ±‚å‰å»¶è¿Ÿï¼Œé¿å…ç¬é—´åŒæ—¶å‘é€
            if task_start_delay > 0:
                _log(f"[Task {idx}] ä»»åŠ¡å†…éƒ¨å»¶è¿Ÿ {task_start_delay:.2f}ç§’åå¼€å§‹æ‰§è¡Œ")
                time.sleep(task_start_delay)
            
            attempts = 0
            last_error = None
            while attempts <= retry_times:
                if time.time() - start_time > total_timeout:
                    return {
                        "index": idx,
                        "status": "timeout_total",
                        "error": f"è¶…è¿‡æ€»è¶…æ—¶æ—¶é—´ {total_timeout}s",
                        "response": None,
                    }
                attempts += 1
                try:
                    # ä¸ºå½“å‰ä»»åŠ¡æ„å»º payload å‰¯æœ¬ï¼ˆåŒ…å«æ‰€æœ‰è¾“å…¥å›¾ç‰‡ï¼‰
                    payload = json.loads(json.dumps(base_payload, ensure_ascii=False))
                    parts_local = [p.copy() for p in base_parts]

                    # å°†æ‰€æœ‰ image_urls ä½œä¸ºå¤šå›¾è¾“å…¥é™„åŠ åˆ°åŒä¸€ä¸ªä»»åŠ¡ä¸­ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
                    # ğŸ”§ ä¼˜åŒ–ï¼šå¦‚æœé¢„ä¸‹è½½å·²å®Œæˆï¼Œç›´æ¥ä»ç¼“å­˜è·å–ï¼Œé¿å…é‡å¤æ£€æŸ¥
                    for url in image_urls:
                        try:
                            # ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤ä¸‹è½½ï¼ˆå¦‚æœé¢„ä¸‹è½½å·²å®Œæˆï¼Œè¿™é‡Œåº”è¯¥ç›´æ¥ä»ç¼“å­˜è·å–ï¼‰
                            img_bytes = _download_image(url, proxies=proxies, timeout=single_timeout, use_cache=True)
                            if img_bytes:
                                b64_img = base64.b64encode(img_bytes).decode()
                                parts_local.append({
                                    "inlineData": {
                                        "mimeType": image_mime or "image/png",
                                        "data": b64_img
                                    }
                                })
                            else:
                                _log(f"[Task {idx}] âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥æˆ–ä¸ºç©º: {url}")
                        except Exception as e:
                            _log(f"[Task {idx}] ä¸‹è½½å›¾ç‰‡å¤±è´¥: {url} -> {e}")
                    payload["contents"][0]["parts"] = parts_local

                    _log(f"[Task {idx}] Request URL: {endpoint}")
                    logged_headers = headers.copy()
                    if "Authorization" in logged_headers:
                        logged_headers["Authorization"] = "Bearer sk-..."
                    if "x-goog-api-key" in logged_headers:
                        logged_headers["x-goog-api-key"] = "AIzaSy..."
                    _log(f"[Task {idx}] Request Headers: {logged_headers}")
                    _log(f"[Task {idx}] Request Payload: {json.dumps(_redact_for_log(payload), ensure_ascii=False, indent=2)}")

                    resp = requests.post(
                        endpoint,
                        headers=headers,
                        data=json.dumps(payload),
                        timeout=single_timeout,
                    )

                    _log(f"[Task {idx}] Response Status Code: {resp.status_code}")
                    if resp.status_code != 200:
                        last_error = f"HTTP {resp.status_code}: {resp.text}"
                        _log(f"[Task {idx}] Error: {last_error}")
                        continue

                    resp_json = resp.json()
                    _log(f"[Task {idx}] Response Body: {json.dumps(_redact_for_log(resp_json), ensure_ascii=False, indent=2)}")
                    status = "success"
                    return {
                        "index": idx,
                        "status": status,
                        "error": None,
                        "response": resp_json,
                    }

                except requests.exceptions.Timeout as e:
                    last_error = f"è¯·æ±‚è¶…æ—¶ (single_timeout={single_timeout}s): {e}"
                    _log(f"[Task {idx}] {last_error}")
                    continue
                except requests.exceptions.SSLError as e:
                    last_error = f"SSLè¿æ¥é”™è¯¯: {e}"
                    _log(f"[Task {idx}] {last_error}")
                    continue
                except requests.exceptions.ProxyError as e:
                    last_error = f"ä»£ç†è¿æ¥é”™è¯¯: {e}"
                    _log(f"[Task {idx}] {last_error}")
                    continue
                except requests.exceptions.ConnectionError as e:
                    last_error = f"ç½‘ç»œè¿æ¥é”™è¯¯: {e}"
                    _log(f"[Task {idx}] {last_error}")
                    continue
                except Exception as e:
                    last_error = f"è¯·æ±‚å¤±è´¥: {e}"
                    _log(f"[Task {idx}] {last_error}")
                    import traceback
                    _log(traceback.format_exc())
                    continue

            return {
                "index": idx,
                "status": "error",
                "error": last_error or "æœªçŸ¥é”™è¯¯",
                "response": None,
            }

        # æ„å»ºä»»åŠ¡åˆ—è¡¨ï¼šä¸€ä¸ªä»»åŠ¡ = ä¸€æ¬¡å®Œæ•´ Banana è°ƒç”¨ï¼ˆå¤šå›¾è¾“å…¥åœ¨åŒä¸€ä»»åŠ¡ä¸­ï¼‰
        # å¹¶å‘æ•° = åŒæ—¶è·‘å¤šå°‘ä¸ªç‹¬ç«‹ä»»åŠ¡
        total_tasks = max(1, int(concurrency) if isinstance(concurrency, (int, float)) else 1)
        tasks = list(range(total_tasks))
        start_time = time.time()

        results = []
        max_workers = max(1, min(total_tasks, len(tasks)))
        
        _log(f"ğŸš€ å¼€å§‹å¹¶å‘æ‰§è¡Œ {total_tasks} ä¸ªä»»åŠ¡ï¼Œè¯·æ±‚é—´éš”: {request_delay}ç§’")
        
        # ğŸ”§ ä¼˜åŒ–ï¼šå…ˆé¢„ä¸‹è½½æ‰€æœ‰å›¾ç‰‡ï¼ˆé¿å…æ¯ä¸ªä»»åŠ¡é‡å¤ä¸‹è½½ï¼‰
        # æ³¨æ„ï¼šé¢„ä¸‹è½½æ˜¯å¼‚æ­¥çš„ï¼Œä¸é˜»å¡ä»»åŠ¡æäº¤
        pre_download_complete = False
        if image_urls:
            _log(f"ğŸ“¥ å¼€å§‹é¢„ä¸‹è½½ {len(image_urls)} å¼ å›¾ç‰‡ï¼ˆå¹¶è¡Œä¸‹è½½ï¼Œä½¿ç”¨ç¼“å­˜ï¼‰...")
            pre_download_start = time.time()

            def download_one(url):
                try:
                    img_data = _download_image(url, proxies=None, timeout=single_timeout, use_cache=True)
                    return img_data is not None
                except Exception as e:
                    _log(f"é¢„ä¸‹è½½å¤±è´¥: {url} -> {e}")
                    return False

            # å¹¶è¡Œä¸‹è½½æ‰€æœ‰å›¾ç‰‡ï¼ˆä½¿ç”¨ç¼“å­˜æœºåˆ¶é¿å…é‡å¤ä¸‹è½½ï¼‰
            with ThreadPoolExecutor(max_workers=min(len(image_urls), 5)) as download_executor:
                download_futures = [download_executor.submit(download_one, url) for url in image_urls]
                success_count = sum(1 for future in download_futures if future.result())

            pre_download_elapsed = time.time() - pre_download_start
            _log(f"âœ… é¢„ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸ: {success_count}/{len(image_urls)} å¼ ï¼ˆå·²ç¼“å­˜ï¼‰ï¼Œè€—æ—¶: {pre_download_elapsed:.2f}ç§’")
            pre_download_complete = True
        
        # ğŸ”§ ç«‹å³å¯åŠ¨å¹¶å‘ä»»åŠ¡ï¼ˆä¸ç­‰å¾…é¢„ä¸‹è½½å®Œæˆï¼Œä»»åŠ¡å†…éƒ¨ä¼šä½¿ç”¨ç¼“å­˜ï¼‰
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_idx = {}
            task_start_times = {}
            
            # ğŸ”§ ç«‹å³æäº¤æ‰€æœ‰ä»»åŠ¡åˆ°çº¿ç¨‹æ± ï¼ˆçœŸæ­£å¹¶å‘ï¼‰
            # å»¶è¿Ÿåœ¨ä»»åŠ¡å†…éƒ¨æ‰§è¡Œï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
            for idx in tasks:
                task_delay = idx * request_delay if idx > 0 and request_delay > 0 else 0
                task_start_times[idx] = time.time()
                future = executor.submit(_call_single, idx, start_time, task_start_delay=task_delay)
                future_to_idx[future] = idx
            
            submit_end_time = time.time()
            submit_elapsed = submit_end_time - start_time
            _log(f"ğŸ“Š æ‰€æœ‰ {total_tasks} ä¸ªä»»åŠ¡å·²æäº¤åˆ°çº¿ç¨‹æ± ï¼ˆè€—æ—¶: {submit_elapsed:.3f}ç§’ï¼‰")
            if request_delay > 0:
                _log(f"   æ¯ä¸ªä»»åŠ¡å†…éƒ¨å»¶è¿Ÿ: Task0=0s, Task1={request_delay:.2f}s, Task2={request_delay*2:.2f}s...")
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            completed_count = 0
            first_complete_time = None
            for future in as_completed(future_to_idx):
                res = future.result()
                results.append(res)
                completed_count += 1
                elapsed = time.time() - start_time
                
                # è®°å½•ç¬¬ä¸€ä¸ªä»»åŠ¡å®Œæˆæ—¶é—´
                if first_complete_time is None:
                    first_complete_time = time.time()
                    first_task_elapsed = first_complete_time - start_time
                    _log(f"âš¡ ç¬¬ä¸€ä¸ªä»»åŠ¡å®Œæˆ: Task {res.get('index')}ï¼Œè€—æ—¶: {first_task_elapsed:.2f}ç§’")
                
                task_start = task_start_times.get(res.get('index'), start_time)
                task_elapsed = time.time() - task_start
                _log(f"âœ… Task {res.get('index')} å®Œæˆ ({completed_count}/{total_tasks})ï¼Œæ€»è€—æ—¶: {elapsed:.2f}ç§’ï¼Œä»»åŠ¡æ‰§è¡Œè€—æ—¶: {task_elapsed:.2f}ç§’")
            
            total_elapsed = time.time() - start_time
            _log(f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼æ€»è€—æ—¶: {total_elapsed:.2f}ç§’ï¼Œå¹³å‡æ¯ä¸ªä»»åŠ¡: {total_elapsed/total_tasks:.2f}ç§’")

        # æŒ‰ index æ’åºï¼Œä¿è¯é¡ºåºç¨³å®š
        results.sort(key=lambda x: x.get("index", 0))

        responses = [r.get("response") for r in results]
        statuses = [r.get("status") for r in results]

        # ä»æ¯ä¸ªå“åº”ä¸­æå–æ‰€æœ‰å›¾åƒï¼Œæ„å»ºé€ä»»åŠ¡çš„å›¾åƒåˆ—è¡¨ä¸æ•°æ®URL
        image_tensors = []
        image_task_index = []  # ç”¨äºè®°å½•æ¯å¼ å›¾ç‰‡å±äºå“ªä¸ªå¹¶å‘ä»»åŠ¡
        image_urls_output = [[] for _ in range(total_tasks)]
        for r in results:
            idx = r.get("index", 0)
            if idx < 0 or idx >= len(image_urls_output):
                continue
            resp_json = r.get("response")
            if not isinstance(resp_json, dict):
                continue
            try:
                extracted_images = _extract_response_images(
                    resp_json,
                    strict_native=strict_native,
                    timeout=single_timeout,
                )
            except Exception as e:
                _log(f"Error extracting image from response index {idx}: {e}")
                extracted_images = []

            for img_info in extracted_images:
                img_bytes = img_info.get("bytes")
                mime_type = img_info.get("mime") or "image/png"
                if not img_bytes:
                    continue

                try:
                    pil = Image.open(BytesIO(img_bytes))
                    _log(f"Decoded image index={idx} mode={pil.mode} size={pil.size}")
                    pil = pil.convert("RGB")
                except Exception as e:
                    _log(f"PIL open/convert failed for index {idx}: {e}")
                    try:
                        pil = Image.open(BytesIO(img_bytes)).convert("RGB")
                    except Exception as e2:
                        _log(f"PIL retry failed for index {idx}: {e2}")
                        continue

                arr = np.array(pil)
                img_t = torch.from_numpy(arr).float() / 255.0
                if img_t.dim() == 3:
                    img_t = img_t.unsqueeze(0)
                image_tensors.append(img_t)
                image_task_index.append(idx)

                url_str = img_info.get("url")
                if not url_str:
                    try:
                        encoded = base64.b64encode(img_bytes).decode("utf-8")
                        url_str = f"data:{mime_type};base64,{encoded}"
                    except Exception:
                        url_str = None
                if url_str:
                    image_urls_output[idx].append(url_str)

        has_real_images = bool(image_tensors)
        if has_real_images:
            images_out = torch.cat(image_tensors, dim=0)
        else:
            images_out = torch.zeros(1, 512, 512, 3)

        # è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºæœ€ç»ˆ IMAGE å¼ é‡çš„å½¢çŠ¶ä¸æ•°å€¼èŒƒå›´ï¼Œæ–¹ä¾¿æ’æŸ¥ ComfyUI æ˜¾ç¤ºé—®é¢˜
        try:
            _log(
                f"images_out shape={tuple(images_out.shape)}, "
                f"dtype={getattr(images_out, 'dtype', None)}, "
                f"min={float(images_out.min())}, max={float(images_out.max())}"
            )
        except Exception as _e:
            _log(f"images_out debug log failed: {_e}")

        # å…ˆåŸºäºåŸå§‹ HTTP URL ç”Ÿæˆ valid_urls
        valid_urls = [url for group in image_urls_output for url in group]

        # å¦‚æœå¯ç”¨ OSS ä¸Šä¼ ï¼Œå¹¶ä¸”æœ‰çœŸå®å›¾åƒï¼Œåˆ™ä¼˜å…ˆç”¨ OSS URL è¦†ç›– image_urls_output / valid_urls
        if oss_enable_upload and has_real_images and OSSUploadFromData:
            oss_endpoint = (oss_endpoint or "").strip()
            oss_access_key_id = (oss_access_key_id or "").strip()
            oss_access_key_secret = (oss_access_key_secret or "").strip()
            oss_bucket_name = (oss_bucket_name or "").strip()
            oss_object_prefix = (oss_object_prefix or "uploads/").strip() or "uploads/"
            oss_file_name = (oss_file_name or "").strip()
            oss_mime_type = (oss_mime_type or "").strip()
            oss_security_token = (oss_security_token or "").strip()
            expire_seconds = int(max(60, oss_signed_url_expire_seconds or 3600))

            if not (oss_endpoint and oss_access_key_id and oss_access_key_secret and oss_bucket_name):
                _log("âš ï¸ OSSä¸Šä¼ è¢«è·³è¿‡ï¼šç¼ºå°‘å¿…è¦é…ç½®ï¼ˆendpoint/access_key/bucketï¼‰")
            else:
                try:
                    uploader = OSSUploadFromData()
                    upload_tensor = images_out
                    upload_result = uploader.upload(
                        endpoint=oss_endpoint,
                        access_key_id=oss_access_key_id,
                        access_key_secret=oss_access_key_secret,
                        bucket_name=oss_bucket_name,
                        object_prefix=oss_object_prefix,
                        use_signed_url=bool(oss_use_signed_url),
                        signed_url_expire_seconds=expire_seconds,
                        images=upload_tensor,
                        image=None,
                        audio=None,
                        audios=None,
                        video=None,
                        videos=None,
                        file_name=oss_file_name or "",
                        mime_type=oss_mime_type or "",
                        security_token=oss_security_token,
                    )
                    if isinstance(upload_result, tuple) and upload_result:
                        urls = upload_result[0]
                        if isinstance(urls, list) and len(urls) == len(image_task_index):
                            _log(f"âœ… OSSä¸Šä¼ æˆåŠŸï¼Œè¿”å› {len(urls)} ä¸ªURL")
                            # é‡æ–°æ„å»ºæŒ‰ä»»åŠ¡åˆ†ç»„çš„ image_urls_output
                            image_urls_output = [[] for _ in range(total_tasks)]
                            for img_idx, url in enumerate(urls):
                                t_idx = image_task_index[img_idx]
                                if 0 <= t_idx < len(image_urls_output):
                                    image_urls_output[t_idx].append(url)
                            valid_urls = [url for group in image_urls_output for url in group]
                except Exception as e:
                    _log(f"âš ï¸ OSSä¸Šä¼ å¤±è´¥: {e}")

        # å¦‚æœä»»åŠ¡æˆåŠŸä½†æ²¡æœ‰è¿”å›å›¾ç‰‡ï¼Œè°ƒæ•´çŠ¶æ€
        for i, r in enumerate(results):
            idx = r.get("index", i)
            if idx < 0 or idx >= len(image_urls_output):
                continue
            if statuses[i] == "success" and not image_urls_output[idx]:
                statuses[i] = "no_image"

        return (
            json.dumps(responses, ensure_ascii=False),
            json.dumps(statuses, ensure_ascii=False),
            json.dumps(image_urls_output, ensure_ascii=False),
            json.dumps(valid_urls, ensure_ascii=False),
            images_out,
        )


NODE_CLASS_MAPPINGS = {
    "Banana2Concurrent": Banana2ConcurrentNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Banana2Concurrent": "AIYang007_banana2_Concurrent",
}

